{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIbwwnInMgjF",
    "outputId": "860a229f-5b74-4ef7-8f35-e9144ad14ea3",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!pip install rembg\n",
    "from IPython.display import display, Javascript, Image\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode, b64encode\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "import io\n",
    "import html\n",
    "import time\n",
    "import subprocess\n",
    "from PIL import Image as PILImage, ImageEnhance\n",
    "import cv2\n",
    "import rembg\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Function to convert the JavaScript object into an OpenCV image\n",
    "def js_to_image(js_reply):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "            js_reply: JavaScript object containing an image from the webcam\n",
    "    Returns:\n",
    "            img: OpenCV BGR image\n",
    "    \"\"\"\n",
    "    # Decode base64 image\n",
    "    image_bytes = b64decode(js_reply.split(',')[1])\n",
    "    # Convert bytes to a numpy array\n",
    "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "    # Decode the numpy array into an OpenCV BGR image\n",
    "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
    "\n",
    "    return img\n",
    "\n",
    "# Function to convert an OpenCV Rectangle bounding box image into a base64 byte string to be overlayed on a video stream\n",
    "def bbox_to_bytes(bbox_array):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "            bbox_array: Numpy array (pixels) containing a rectangle to overlay on the video stream.\n",
    "    Returns:\n",
    "        bytes: Base64 image byte string\n",
    "    \"\"\"\n",
    "    # Convert the array into a PIL image\n",
    "    bbox_PIL = PILImage.fromarray(bbox_array, 'RGBA')\n",
    "    iobuf = io.BytesIO()\n",
    "    # Format the bounding box into PNG for return\n",
    "    bbox_PIL.save(iobuf, format='png')\n",
    "    # Format the return string\n",
    "    bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
    "\n",
    "    return bbox_bytes\n",
    "\n",
    "# Function to capture a photo and remove the background using rembg\n",
    "def capture_and_remove_background(background_path):\n",
    "    try:\n",
    "        # Capture a photo\n",
    "        def take_photo(filename='photo.jpg', quality=0.8):\n",
    "            js = Javascript('''\n",
    "              async function takePhoto(quality) {\n",
    "                const div = document.createElement('div');\n",
    "                const capture = document.createElement('button');\n",
    "                capture.textContent = 'Capture';\n",
    "                div.appendChild(capture);\n",
    "\n",
    "                const video = document.createElement('video');\n",
    "                video.style.display = 'block';\n",
    "                const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "                document.body.appendChild(div);\n",
    "                div.appendChild(video);\n",
    "                video.srcObject = stream;\n",
    "                await video.play();\n",
    "\n",
    "                // Resize the output to fit the video element.\n",
    "                google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "                // Wait for Capture to be clicked.\n",
    "                await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "                const canvas = document.createElement('canvas');\n",
    "                canvas.width = video.videoWidth;\n",
    "                canvas.height = video.videoHeight;\n",
    "                canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "                stream.getVideoTracks()[0].stop();\n",
    "                div.remove();\n",
    "                return canvas.toDataURL('image/jpeg', quality);\n",
    "              }\n",
    "              ''')\n",
    "            display(js)\n",
    "\n",
    "            # Get photo data\n",
    "            data = eval_js('takePhoto({})'.format(quality))\n",
    "            # Get the OpenCV format image\n",
    "            img = js_to_image(data)\n",
    "\n",
    "            cv2.imwrite(filename, img)\n",
    "\n",
    "            return filename\n",
    "\n",
    "        # Capture a photo and save only the object as 'photo.jpg'\n",
    "        filename = take_photo('photo.jpg')\n",
    "        print('Saved to {}'.format(filename))\n",
    "\n",
    "        # Remove the background using rembg and overwrite 'photo.jpg' with the object\n",
    "        remove_background('photo.jpg', 'photo.jpg')\n",
    "\n",
    "        # Overlay the object on the specified background\n",
    "        overlay_photo('photo.jpg', background_path)\n",
    "\n",
    "        # Display the composed image with the new background\n",
    "        display(Image('photo.jpg'))\n",
    "\n",
    "    except Exception as err:\n",
    "        # Handle any errors\n",
    "        print(str(err))\n",
    "\n",
    "# Function to remove the background using rembg\n",
    "def remove_background(input_path, output_path):\n",
    "    try:\n",
    "        # Use rembg to remove the background\n",
    "        subprocess.run([\"rembg\", input_path, output_path])\n",
    "        print('Background removed and object saved to {}'.format(output_path))\n",
    "    except Exception as err:\n",
    "        print('Error:', str(err))\n",
    "\n",
    "# Function to overlay the object on a different background\n",
    "def overlay_photo(photo_path, background_path):\n",
    "    try:\n",
    "        # Open the captured image with rembg\n",
    "        with PILImage.open(photo_path) as img:\n",
    "            img = img.convert('RGBA')\n",
    "            with rembg.remove(img) as output:\n",
    "                output.save(photo_path, 'PNG')\n",
    "\n",
    "        # Open the background image\n",
    "        captured_image = PILImage.open(photo_path)\n",
    "        background_image = PILImage.open(background_path)\n",
    "\n",
    "        # Adjust contrast\n",
    "        contrast = ImageEnhance.Contrast(captured_image)\n",
    "        captured_image = contrast.enhance(1.0)  # You can adjust the factor (e.g., 2.0) as needed\n",
    "\n",
    "        # Adjust sharpness\n",
    "        sharpness = ImageEnhance.Sharpness(captured_image)\n",
    "        captured_image = sharpness.enhance(2.0)  # You can adjust the factor (e.g., 2.0) as needed\n",
    "\n",
    "        brightness = ImageEnhance.Brightness(background_image)\n",
    "        background_image = brightness.enhance(0.7)\n",
    "\n",
    "        captured_image = captured_image.resize(background_image.size)\n",
    "        composed_image = PILImage.alpha_composite(background_image.convert('RGBA'), captured_image.convert('RGBA'))\n",
    "\n",
    "        composed_image_path = f'composed_photo.png'\n",
    "        composed_image.save(composed_image_path, 'PNG')\n",
    "\n",
    "        print('Composed photo saved as {}'.format(composed_image_path))\n",
    "\n",
    "    except Exception as err:\n",
    "        print('Error:', str(err))\n",
    "\n",
    "# Specify the path to your desired background image\n",
    "background_image_path = '/content/drive/MyDrive/flask app/7.jpg'\n",
    "\n",
    "# Capture a photo, remove the background, and overlay on the specified background\n",
    "#capture_and_remove_background(background_image_path)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (10,10)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "#%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "from tensorflow.python.keras import models\n",
    "\n",
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive. mount('/content/drive')\n",
    "\n",
    "def load_img(path_to_img):\n",
    "  max_dim = 512\n",
    "  img = Image.open(path_to_img)\n",
    "  long = max(img.size)\n",
    "  scale = max_dim/long\n",
    "  img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n",
    "\n",
    "  #img = kp_image.img_to_array(img)\n",
    "\n",
    "  # We need to broadcast the image array such that it has a batch dimension\n",
    "  img = np.expand_dims(img, axis=0)\n",
    "  return img\n",
    "\n",
    "def imshow(img, title=None):\n",
    "  # Remove the batch dimension\n",
    "  out = np.squeeze(img, axis=0)\n",
    "  # Normalize for display\n",
    "  out = out.astype('uint8')\n",
    "  plt.imshow(out)\n",
    "  if title is not None:\n",
    "    plt.title(title)\n",
    "  plt.imshow(out)\n",
    "\n",
    "def load_and_process_img(path_to_img):\n",
    "  img = load_img(path_to_img)\n",
    "  img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "  return img\n",
    "\n",
    "def deprocess_img(processed_img):\n",
    "  x = processed_img.copy()\n",
    "  if len(x.shape) == 4:\n",
    "    x = np.squeeze(x, 0)\n",
    "  assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n",
    "                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n",
    "  if len(x.shape) != 3:\n",
    "    raise ValueError(\"Invalid input to deprocessing image\")\n",
    "\n",
    "  # perform the inverse of the preprocessing step\n",
    "  x[:, :, 0] += 103.939\n",
    "  x[:, :, 1] += 116.779\n",
    "  x[:, :, 2] += 123.68\n",
    "  x = x[:, :, ::-1]\n",
    "\n",
    "  x = np.clip(x, 0, 255).astype('uint8')\n",
    "  return x\n",
    "\n",
    "# Content layer where will pull our feature maps\n",
    "content_layers = ['block5_conv2']\n",
    "\n",
    "# Style layer we are interested in\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1'\n",
    "               ]\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)\n",
    "\n",
    "def get_model():\n",
    "  \"\"\" Creates our model with access to intermediate layers.\n",
    "\n",
    "  This function will load the VGG19 model and access the intermediate layers.\n",
    "  These layers will then be used to create a new model that will take input image\n",
    "  and return the outputs from these intermediate layers from the VGG model.\n",
    "\n",
    "  Returns:\n",
    "    returns a keras model that takes image inputs and outputs the style and\n",
    "      content intermediate layers.\n",
    "  \"\"\"\n",
    "  # Load our model. We load pretrained VGG, trained on imagenet data\n",
    "  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "  vgg.trainable = False\n",
    "  # Get output layers corresponding to style and content layers\n",
    "  style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
    "  content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
    "  model_outputs = style_outputs + content_outputs\n",
    "  # Build model\n",
    "  return models.Model(vgg.input, model_outputs)\n",
    "\n",
    "def get_content_loss(base_content, target):\n",
    "  return tf.reduce_mean(tf.square(base_content - target))\n",
    "\n",
    "def gram_matrix(input_tensor):\n",
    "  # We make the image channels first\n",
    "  channels = int(input_tensor.shape[-1])\n",
    "  a = tf.reshape(input_tensor, [-1, channels])\n",
    "  n = tf.shape(a)[0]\n",
    "  gram = tf.matmul(a, a, transpose_a=True)\n",
    "  return gram / tf.cast(n, tf.float32)\n",
    "\n",
    "def get_style_loss(base_style, gram_target):\n",
    "  \"\"\"Expects two images of dimension h, w, c\"\"\"\n",
    "  # height, width, num filters of each layer\n",
    "  # We scale the loss at a given layer by the size of the feature map and the number of filters\n",
    "  height, width, channels = base_style.get_shape().as_list()\n",
    "  gram_style = gram_matrix(base_style)\n",
    "\n",
    "  return tf.reduce_mean(tf.square(gram_style - gram_target))# / (4. * (channels ** 2) * (width * height) ** 2)\n",
    "\n",
    "def compute_loss(outputs, loss_weights, init_image, gram_style_matrices, content_features):\n",
    "    style_weight, content_weight = loss_weights\n",
    "\n",
    "    style_outputs = outputs[:num_style_layers]\n",
    "    content_outputs = outputs[num_style_layers:]\n",
    "\n",
    "    style_loss = tf.add_n([get_style_loss(style_output[0], target_gram) for style_output, target_gram in zip(style_outputs, gram_style_matrices)])\n",
    "\n",
    "    content_loss = tf.add_n([get_content_loss(content_output[0], target_content) for content_output, target_content in zip(content_outputs, content_features)])\n",
    "\n",
    "    style_loss *= style_weight\n",
    "    content_loss *= content_weight\n",
    "\n",
    "    total_loss = style_loss + content_loss\n",
    "\n",
    "    return total_loss, style_loss, content_loss\n",
    "\n",
    "\n",
    "def get_feature_representations(model, content_path, style_path):\n",
    "  \"\"\"Helper function to compute our content and style feature representations.\n",
    "\n",
    "  This function will simply load and preprocess both the content and style\n",
    "  images from their path. Then it will feed them through the network to obtain\n",
    "  the outputs of the intermediate layers.\n",
    "\n",
    "  Arguments:\n",
    "    model: The model that we are using.\n",
    "    content_path: The path to the content image.\n",
    "    style_path: The path to the style image\n",
    "\n",
    "  Returns:\n",
    "    returns the style features and the content features.\n",
    "  \"\"\"\n",
    "  # Load our images in\n",
    "  content_image = load_and_process_img(content_path)\n",
    "  style_image = load_and_process_img(style_path)\n",
    "\n",
    "  # batch compute content and style features\n",
    "  style_outputs = model(style_image)\n",
    "  content_outputs = model(content_image)\n",
    "\n",
    "\n",
    "  # Get the style and content feature representations from our model\n",
    "  style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\n",
    "  content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]\n",
    "  return style_features, content_features\n",
    "\n",
    "def compute_loss_with_precomputed_style(style_features, model, loss_weights, init_image, content_features):\n",
    "    style_weight, content_weight = loss_weights\n",
    "\n",
    "    # Feed our init image through our model. This will give us the content representations.\n",
    "    model_outputs = model(init_image)\n",
    "    content_output_features = model_outputs[num_style_layers:]\n",
    "\n",
    "    content_score = 0\n",
    "\n",
    "    # Compute content loss from all layers\n",
    "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
    "    for target_content, comb_content in zip(content_features, content_output_features):\n",
    "        content_score += weight_per_content_layer * get_content_loss(comb_content[0], target_content)\n",
    "\n",
    "    content_score *= content_weight\n",
    "\n",
    "    # Get total loss\n",
    "    loss = content_score\n",
    "\n",
    "    return loss, content_score\n",
    "\n",
    "\n",
    "def compute_grads(cfg):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = cfg['model'](cfg['init_image'])\n",
    "        all_loss = compute_loss(outputs, cfg['loss_weights'], cfg['init_image'], cfg['gram_style_features'], cfg['content_features'])\n",
    "    total_loss = all_loss[0]\n",
    "    return tape.gradient(total_loss, cfg['init_image']), all_loss\n",
    "\n",
    "\n",
    "import IPython.display\n",
    "\n",
    "def run_style_transfer(content_path,\n",
    "                       style_path,\n",
    "                       num_iterations=500,\n",
    "                       content_weight=1e3,\n",
    "                       style_weight=1e-2):\n",
    "  # We don't need to (or want to) train any layers of our model, so we set their\n",
    "  # trainable to false.\n",
    "  model = get_model()\n",
    "  for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "  # Get the style and content feature representations (from our specified intermediate layers)\n",
    "  style_features, content_features = get_feature_representations(model, content_path, style_path)\n",
    "  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
    "\n",
    "  # Set initial image\n",
    "  init_image = load_and_process_img(content_path)\n",
    "  init_image = tf.Variable(init_image, dtype=tf.float32)\n",
    "  # Create our optimizer\n",
    "  opt = tf.optimizers.Adam(learning_rate=2, epsilon=1e-1)\n",
    "\n",
    "  # For displaying intermediate images\n",
    "  iter_count = 10\n",
    "\n",
    "  # Store our best result\n",
    "  best_loss, best_img = float('inf'), None\n",
    "\n",
    "  # Create a nice config\n",
    "  loss_weights = (style_weight, content_weight)\n",
    "  cfg = {\n",
    "    'model': model,  # Modeli cfg sözlüğüne ekleyin\n",
    "    'loss_weights': loss_weights,\n",
    "    'init_image': init_image,\n",
    "    'gram_style_features': gram_style_features,\n",
    "    'content_features': content_features\n",
    "  }\n",
    "  # For displaying\n",
    "  num_rows = 2\n",
    "  num_cols = 5\n",
    "  display_interval = num_iterations/(num_rows*num_cols)\n",
    "  start_time = time.time()\n",
    "  global_start = time.time()\n",
    "\n",
    "  norm_means = np.array([103.939, 116.779, 123.68])\n",
    "  min_vals = -norm_means\n",
    "  max_vals = 255 - norm_means\n",
    "\n",
    "  imgs = []\n",
    "  for i in range(num_iterations):\n",
    "    grads, all_loss = compute_grads(cfg)\n",
    "    loss, style_score, content_score = all_loss\n",
    "    opt.apply_gradients([(grads, init_image)])\n",
    "    clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n",
    "    init_image.assign(clipped)\n",
    "    end_time = time.time()\n",
    "    print(i)\n",
    "    if loss < best_loss:\n",
    "      # Update best loss and best image from total loss.\n",
    "      best_loss = loss\n",
    "      best_img = deprocess_img(init_image.numpy())\n",
    "\n",
    "    if i % display_interval== 0:\n",
    "      start_time = time.time()\n",
    "\n",
    "      # Use the .numpy() method to get the concrete numpy array\n",
    "      plot_img = init_image.numpy()\n",
    "      plot_img = deprocess_img(plot_img)\n",
    "      imgs.append(plot_img)\n",
    "      IPython.display.clear_output(wait=True)\n",
    "      IPython.display.display_png(Image.fromarray(plot_img))\n",
    "\n",
    "      print('Iteration: {}'.format(i))\n",
    "      print('Total loss: {:.4e}, '\n",
    "            'style loss: {:.4e}, '\n",
    "            'content loss: {:.4e}, '\n",
    "            'time: {:.4f}s'.format(loss, style_score, content_score, time.time() - start_time))\n",
    "  print('Total time: {:.4f}s'.format(time.time() - global_start))\n",
    "  IPython.display.clear_output(wait=True)\n",
    "  plt.figure(figsize=(14,4))\n",
    "  for i,img in enumerate(imgs):\n",
    "      plt.subplot(num_rows,num_cols,i+1)\n",
    "      plt.imshow(img)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "\n",
    "  return best_img, best_loss\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def png_to_jpg(input_path, output_path, quality=95):\n",
    "\n",
    "    try:\n",
    "\n",
    "        image = Image.open(input_path)\n",
    "\n",
    "        image = image.convert('RGB')  # PNG'nin şeffaf piksellerini beyaza dönüştürmek için\n",
    "\n",
    "        image.save(output_path, 'JPEG', quality=quality)\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Hata oluştu: {e}\")\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "# Kullanımı:\n",
    "\n",
    "#input_path = '/content/composed_photo.png'  # Dönüştürmek istediğiniz PNG dosyasının yolu\n",
    "\n",
    "#output_path = 'cikis.jpg'  # Dönüşen JPG dosyasının kaydedileceği yol\n",
    "\n",
    "#png_to_jpg(input_path, output_path)\n",
    "\n",
    "#best, best_loss = run_style_transfer('/content/cikis.jpg',\n",
    "#                                     '/content/drive/MyDrive/flask app/ulaş style.png', num_iterations=500)\n",
    "\n",
    "#Image.fromarray(best)\n",
    "def tensor_to_image(tensor):\n",
    "  tensor = tensor\n",
    "  tensor = np.array(tensor, dtype=np.uint8)\n",
    "  if np.ndim(tensor)>3:\n",
    "    assert tensor.shape[0] == 1\n",
    "    tensor = tensor[0]\n",
    "  return PIL.Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gelshKz_VYke",
    "outputId": "0c5d365f-ce3e-4b90-84b9-e2cded9035ec"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
      "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.0.0)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.3.7)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.3)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
      "Requirement already satisfied: flask_ngrok in /usr/local/lib/python3.10/dist-packages (0.0.25)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.3.7)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask_ngrok) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask_ngrok) (2023.7.22)\n",
      "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!pip install flask pyngrok\n",
    "!pip install flask flask_ngrok\n",
    "!ngrok authtoken 2Vfwbr8lLhZJjCQnGm8LB7KiXET_4JSFgnkiNKFt7RdTzszJs"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install qrcode"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I67SzalX2iqn",
    "outputId": "e0e7d5f0-d020-414f-efa3-22761bef7ad3"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: qrcode in /usr/local/lib/python3.10/dist-packages (7.4.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qrcode) (4.5.0)\n",
      "Requirement already satisfied: pypng in /usr/local/lib/python3.10/dist-packages (from qrcode) (0.20220715.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from flask import Flask, render_template, request, redirect, url_for, send_file, session\n",
    "from flask_ngrok import run_with_ngrok\n",
    "import os\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "import qrcode\n",
    "\n",
    "# Set the correct path to your project folder\n",
    "os.chdir(\"/content/drive/MyDrive/flask app\")\n",
    "\n",
    "# Create a Flask app\n",
    "app = Flask(__name__)\n",
    "app.secret_key = 'can20011a'  # Set your secret key\n",
    "app.static_folder = 'static'\n",
    "\n",
    "# Set up ngrok for tunneling to access the app from the internet (only for Google Colab)\n",
    "run_with_ngrok(app)\n",
    "\n",
    "# Initialize background_image_path with a default value\n",
    "app.config['background_image_path'] = None\n",
    "# Initialize style_image_path with a default value\n",
    "app.config['style_image_path'] = None\n",
    "\n",
    "\n",
    "# Açılış sayfası\n",
    "@app.route(\"/\", methods=['GET', 'POST'])\n",
    "def index():\n",
    "      if request.method == 'POST':\n",
    "          if request.form.get('button_a'):\n",
    "              # A butonuna tıklandığında yapılacak işlemler burada\n",
    "              return redirect(url_for('generate_qr_code_homepage'))\n",
    "\n",
    "          elif request.form.get('button_b'):\n",
    "              # B butonuna tıklandığında yapılacak işlemler burada\n",
    "              return redirect(url_for('background'))  # home route'unu yönlendir\n",
    "\n",
    "      return render_template('index.html')  # index.html'i göster\n",
    "\n",
    "\n",
    "# Modify your Flask route to generate and serve the QR code image\n",
    "@app.route(\"/generate_qr_code_homepage\", methods=['GET'])\n",
    "def generate_qr_code_homepage():\n",
    "    # Get the current base URL where your Flask app is running\n",
    "    base_url = request.host_url\n",
    "\n",
    "    # Define the link you want to generate a QR code for (the homepage)\n",
    "    link = base_url\n",
    "\n",
    "    # Generate the QR code\n",
    "    qr = qrcode.QRCode(\n",
    "        version=1,\n",
    "        error_correction=qrcode.constants.ERROR_CORRECT_L,\n",
    "        box_size=10,\n",
    "        border=4,\n",
    "    )\n",
    "    qr.add_data(link)\n",
    "    qr.make(fit=True)\n",
    "\n",
    "    img = qr.make_image(fill_color=\"black\", back_color=\"white\")\n",
    "\n",
    "    # Save the QR code image to a BytesIO object\n",
    "    qr_code_image_io = io.BytesIO()\n",
    "    img.save(qr_code_image_io, \"PNG\")\n",
    "    qr_code_image_io.seek(0)\n",
    "\n",
    "    # Convert the BytesIO object to a base64-encoded string\n",
    "    qr_code_image_base64 = base64.b64encode(qr_code_image_io.read()).decode('utf-8')\n",
    "\n",
    "    return render_template('index.html', qr_code_image=qr_code_image_base64)\n",
    "\n",
    "\n",
    "  # Define a route for the home page\n",
    "@app.route(\"/background\", methods=['GET', 'POST'])\n",
    "def home():\n",
    "      if request.method == 'POST':\n",
    "          try:\n",
    "              # Get the selected background image path from the form\n",
    "              background_path = request.form.get('background_path')\n",
    "\n",
    "              # Store the selected background image path in the session\n",
    "              session['background_image_path'] = background_path\n",
    "\n",
    "              # Redirect to the style_selection route\n",
    "              return redirect(url_for('style_selection'))\n",
    "          except Exception as e:\n",
    "              return f\"Error: {str(e)}\"\n",
    "\n",
    "      # List all image files in the /static/Background directory\n",
    "      background_images = []\n",
    "      background_directory = \"/content/drive/MyDrive/flask app/static/Background\"\n",
    "      for filename in os.listdir(background_directory):\n",
    "          if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "              image_path = os.path.join(background_directory, filename)\n",
    "              background_images.append(image_path)\n",
    "\n",
    "      # Render the home.html template with background_images\n",
    "      return render_template('home.html', background_images=background_images)\n",
    "\n",
    "  # Define a route for style image selection\n",
    "@app.route(\"/style_selection\", methods=['GET', 'POST'])\n",
    "def style_selection():\n",
    "      if request.method == 'POST':\n",
    "          try:\n",
    "              # Get the selected style image path from the form\n",
    "              style_path = request.form.get('style_path')\n",
    "\n",
    "              # Store the selected style image path in the session\n",
    "              session['style_image_path'] = style_path\n",
    "\n",
    "              # Redirect to the capture route\n",
    "              return redirect(url_for('capture'))\n",
    "          except Exception as e:\n",
    "              return f\"Error: {str(e)}\"\n",
    "\n",
    "      # List all image files in the /static/Style Image directory for style images\n",
    "      style_images = []\n",
    "      style_directory = \"/content/drive/MyDrive/flask app/static/Style Image\"\n",
    "      for filename in os.listdir(style_directory):\n",
    "          if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "              image_path = os.path.join(style_directory, filename)\n",
    "              style_images.append(image_path)\n",
    "\n",
    "      # Render the style_selection.html template with style_images\n",
    "      return render_template('style_selection.html', style_images=style_images)\n",
    "\n",
    "@app.route(\"/capture\", methods=['GET', 'POST'])\n",
    "def capture():\n",
    "      background_image_path = session.get('background_image_path', None)\n",
    "      style_image_path = session.get('style_image_path', None)\n",
    "\n",
    "      if request.method == 'POST' and 'image_data' in request.form:\n",
    "          try:\n",
    "              # Get the captured image data from the POST request\n",
    "              image_data = request.form.get('image_data')\n",
    "\n",
    "              # Check if image data is not empty\n",
    "              if image_data:\n",
    "                  # Decode the base64 image data\n",
    "                  image_bytes = base64.b64decode(image_data.split(',')[1])\n",
    "\n",
    "                  # Create a PIL image from the decoded bytes\n",
    "                  image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "                  # Save the captured image to a JPG file\n",
    "                  captured_image_path = '/content/drive/MyDrive/flask app/captured_photos/captured_photo.jpg'\n",
    "                  image.save(captured_image_path, 'JPEG')  # Save as JPEG format\n",
    "\n",
    "                  # Check if both background_image_path and style_image_path are not None\n",
    "                  if background_image_path and style_image_path:\n",
    "                      # Remove the background and overlay it on the specified background\n",
    "                      composed_photo_path = overlay_photo(captured_image_path, background_image_path)\n",
    "                      png_to_jpg(\"/content/drive/MyDrive/flask app/composed_photo.png\", \"/content/drive/MyDrive/flask app/composed_photo.jpg\", quality=95)\n",
    "                      best, best_loss = run_style_transfer('/content/drive/MyDrive/flask app/composed_photo.jpg',\n",
    "                                          style_image_path, num_iterations=500)\n",
    "\n",
    "                    # Convert and save the stylized image\n",
    "                      stylized_image_path = '/content/drive/MyDrive/flask app/static/stylized-image_new2.jpg'\n",
    "                      Image.fromarray(best).save(stylized_image_path)\n",
    "\n",
    "                      # Redirect to the result route after processing the image\n",
    "                      return redirect(url_for('result'))\n",
    "\n",
    "          except Exception as e:\n",
    "              return f\"Error: {str(e)}\"\n",
    "\n",
    "      # Render the capture.html template when the page is initially loaded or if there's no \"Process\" request\n",
    "      return render_template('capture.html', background_image_path=background_image_path, style_image_path=style_image_path)\n",
    "\n",
    "\n",
    "# Define a route to display the stylized image and provide a download link\n",
    "@app.route(\"/result\")\n",
    "def result():\n",
    "      stylized_image_path = '/content/drive/MyDrive/flask app/static/stylized-image_new2.jpg'\n",
    "\n",
    "      # Render the result.html template with the stylized image path\n",
    "      return render_template('result.html', stylized_image_path=stylized_image_path)\n",
    "\n",
    "@app.route(\"/download_result\")\n",
    "def download_result():\n",
    "      stylized_image_path = '/content/drive/MyDrive/flask app/static/stylized-image_new2.jpg'\n",
    "      return send_file(stylized_image_path, as_attachment=True)\n",
    "\n",
    "@app.route(\"/download_qr_code\")\n",
    "def download_qr_code():\n",
    "      qr_code_file_path = '/content/drive/MyDrive/flask app/qr_codes/qr_code_homepage.png'\n",
    "      return send_file(qr_code_file_path, as_attachment=True)\n",
    "\n",
    "# Run the Flask app\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ],
   "metadata": {
    "id": "Z7EWR0sX1GMe",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2d5e7475-710a-425e-f465-686d9fc19441"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:werkzeug:\u001B[31m\u001B[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001B[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001B[33mPress CTRL+C to quit\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * Running on http://041b-34-74-100-245.ngrok-free.app\n",
      " * Traffic stats available on http://127.0.0.1:4040\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [25/Sep/2023 03:41:13] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Sep/2023 03:41:14] \"\u001B[33mGET /favicon.ico HTTP/1.1\u001B[0m\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Sep/2023 03:41:14] \"GET /generate_qr_code_homepage?button_a= HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Sep/2023 03:41:17] \"GET /background?button_b= HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Sep/2023 03:41:19] \"\u001B[32mPOST /background HTTP/1.1\u001B[0m\" 302 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Sep/2023 03:41:19] \"GET /style_selection HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Sep/2023 03:41:22] \"\u001B[32mPOST /style_selection HTTP/1.1\u001B[0m\" 302 -\n",
      "INFO:werkzeug:127.0.0.1 - - [25/Sep/2023 03:41:22] \"GET /capture HTTP/1.1\" 200 -\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
